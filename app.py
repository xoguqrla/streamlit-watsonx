import os
import logging
import streamlit as st
from dotenv import load_dotenv
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
API_KEY = os.getenv("API_KEY")
PROJECT_ID = os.getenv("PROJECT_ID")
API_URL = os.getenv("URL")

# ğŸ“Œ ë¡œê¹… ì„¤ì •
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)
logging.basicConfig(filename=f"{LOG_DIR}/chatbot.log", level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

st.set_page_config(page_title="Watsonx AI ì±—ë´‡", layout="wide")

# ğŸ“Œ Watsonx LLM íŒŒë¼ë¯¸í„° ì„¤ì •
parameters = {
    GenParams.DECODING_METHOD: DecodingMethods.GREEDY.value,
    GenParams.MIN_NEW_TOKENS: 1,
    GenParams.MAX_NEW_TOKENS: 500,
    GenParams.STOP_SEQUENCES: ["<|endoftext|>"]
}

# ğŸ“Œ ëª¨ë¸ ìƒì„± í•¨ìˆ˜
@st.cache_resource
def create_llm():
    """IBM Watsonx AI ëª¨ë¸ ìƒì„±"""
    if not API_KEY or not PROJECT_ID or not API_URL:
        st.error("ğŸš¨ API í‚¤ ë˜ëŠ” í”„ë¡œì íŠ¸ IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! `.env` íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    try:
        credentials = Credentials(url=API_URL, api_key=API_KEY)
        model = ModelInference(
            model_id="ibm/granite-3-8b-instruct",
            params=parameters,
            credentials=credentials,
            project_id=PROJECT_ID
        )
        return model
    except Exception as e:
        logging.error(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        st.error("âŒ Watsonx ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        return None

# ğŸ“Œ ëª¨ë¸ ë¡œë“œ
if "model" not in st.session_state:
    st.session_state.model = create_llm()

# ğŸ“Œ ì±„íŒ… ë©”ì‹œì§€ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = []

# ğŸ“Œ ì‚¬ìš©ì ì…ë ¥ê°’ ì €ì¥ ê³µê°„
if "user_inputs" not in st.session_state:
    st.session_state.user_inputs = {"ì„±ë³„": None, "ë‚˜ì´": None, "ì¹´í…Œê³ ë¦¬": None, "ì§€ì—­": None}

# ğŸ“Œ Watsonx AI í˜¸ì¶œ í•¨ìˆ˜
def watsonx_ai_api(prompt):
    """Watsonx AIë¡œë¶€í„° ì‘ë‹µì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    if not prompt.strip():
        return "âš ï¸ ì…ë ¥ëœ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    if st.session_state.model is None:
        return "âŒ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    try:
        response = st.session_state.model.generate(prompt=prompt)['results'][0]['generated_text'].strip()
        logging.info(f"ğŸ“¨ ì‚¬ìš©ì ì…ë ¥: {prompt}  | ğŸ“© ëª¨ë¸ ì‘ë‹µ: {response}")
        return response
    except Exception as e:
        logging.error(f"âŒ Watsonx ì‘ë‹µ ì˜¤ë¥˜: {str(e)}")
        return "ğŸš¨ Watsonx AIì™€ í†µì‹  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ğŸ“Œ ì‚¬ì´ë“œë°” UI ì„¤ì •
st.sidebar.markdown("## ğŸ”¹ ì‚¬ìš©ì ì •ë³´ ì…ë ¥")

# ì„±ë³„ ì„ íƒ (ê°€ë¡œ ì •ë ¬)
st.session_state.user_inputs["ì„±ë³„"] = st.sidebar.radio(
    "ì„±ë³„ì„ ì„ íƒí•˜ì„¸ìš”:", ["ë‚¨ì", "ì—¬ì"], horizontal=True
)

# ë‚˜ì´ ì„ íƒ (15~25ì„¸ ë²”ìœ„)
st.session_state.user_inputs["ë‚˜ì´"] = st.sidebar.selectbox(
    "ë‚˜ì´ë¥¼ ì„ íƒí•˜ì„¸ìš”:", list(range(15, 26))
)

# ì¹´í…Œê³ ë¦¬ ì„ íƒ
st.session_state.user_inputs["ì¹´í…Œê³ ë¦¬"] = st.sidebar.selectbox(
    "ê´€ì‹¬ ìˆëŠ” ì •ë³´ë¥¼ ì„ íƒí•˜ì„¸ìš”:", ["ì£¼ê±°", "ì¼ìë¦¬", "ê¸ˆìœµ", "ë³´í—˜", "í•¸ë“œí°", "ì§€ì› ì œë„"]
)

# ì§€ì—­ ì…ë ¥ í•„ë“œ (placeholder ì¶”ê°€)
st.session_state.user_inputs["ì§€ì—­"] = st.sidebar.text_input(
    "ê±°ì£¼ ì§€ì—­ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì„œìš¸, ë¶€ì‚°, ëŒ€êµ¬)", placeholder="ê±°ì£¼ ì§€ì—­ì„ ì…ë ¥í•˜ì„¸ìš”"
)

# ì„¤ì • ì™„ë£Œ ë²„íŠ¼
if st.sidebar.button("ğŸ ì„¤ì • ì™„ë£Œ", use_container_width=True):
    if not st.session_state.user_inputs["ì§€ì—­"]:
        st.sidebar.warning("ğŸš¨ ì§€ì—­ì„ ì…ë ¥í•˜ì„¸ìš”!")
    else:
        st.sidebar.success(
            f"ğŸ {st.session_state.user_inputs['ì§€ì—­']}ì— ê±°ì£¼í•˜ëŠ” {st.session_state.user_inputs['ë‚˜ì´']}ì„¸ {st.session_state.user_inputs['ì„±ë³„']}ë‹˜ì˜ {st.session_state.user_inputs['ì¹´í…Œê³ ë¦¬']} ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤!"
        )

# ğŸ“Œ IBM ë²Œ í…Œë§ˆ ì¶”ê°€ (ìƒë‹¨ ê³ ì •)
st.markdown(
    """
    <div style="text-align:center; margin-top:20px;">
        <h2 style="color:gold;">ğŸ I'll Be your Mommy (IBM) ğŸ</h2>
    </div>
    """,
    unsafe_allow_html=True
)

# ğŸ“Œ ê¸°ì¡´ ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    avatar = "ğŸ" if message["role"] == "assistant" else "ğŸ‘¤"
    bubble_color = "#FFF3CD" if message["role"] == "assistant" else "#E3E3E3"
    st.markdown(
        f"""
        <div style="background-color: {bubble_color}; padding: 10px; border-radius: 15px; margin-bottom: 10px;">
            {avatar} {message['content']}
        </div>
        """,
        unsafe_allow_html=True
    )

# ğŸ“Œ ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("ğŸ’¬ ì¶”ê°€ ì§ˆë¬¸ì´ ìˆë‚˜ìš”?"):
    user_info = f"ì‚¬ìš©ì ì •ë³´: {st.session_state.user_inputs['ì§€ì—­']} ê±°ì£¼ {st.session_state.user_inputs['ë‚˜ì´']}ì„¸ {st.session_state.user_inputs['ì„±ë³„']}"
    full_prompt = f"{user_info}\n\n{st.session_state.user_inputs['ì¹´í…Œê³ ë¦¬']} ê´€ë ¨ ì§ˆë¬¸ì…ë‹ˆë‹¤: {prompt}"
    st.session_state.messages.append({"role": "user", "content": full_prompt})
    st.markdown(f"ğŸ‘¤ {full_prompt}")
    with st.spinner("ğŸ Watsonx AIê°€ ë‹µë³€ì„ ìƒì„± ì¤‘..."):
        response = watsonx_ai_api(full_prompt) + "\n\nğŸ“ ê´€ë ¨ ê¸°ê´€ ë¬¸ì˜: 123-456-7890\nğŸŒ ê³µì‹ í™ˆí˜ì´ì§€: www.example.com"
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.markdown(
            f"""
            <div style="background-color: #FFF3CD; padding: 10px; border-radius: 15px; margin-bottom: 10px;">
                ğŸ {response}
            </div>
            """,
            unsafe_allow_html=True
        )
